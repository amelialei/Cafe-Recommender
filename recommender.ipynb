{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cafe Recommender"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Started by reading in the business dataset from the Yelp academic dataset, filtered for coffee and tea business, and dropped irrelevant columns. Then read in the reviews dataset while merging the data with the business data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in business data\n",
    "business_json_path = '/Users/amelialei/yelp_dataset/yelp_academic_dataset_business.json'\n",
    "bus_df = pd.read_json(business_json_path, lines = True)\n",
    "\n",
    "\n",
    "# Filter dataset for coffee and tea establishments\n",
    "bus_df = bus_df[bus_df['categories'].str.contains('Bubble Tea|Coffee and Tea', case = False, na = False)] \n",
    "\n",
    "\n",
    "# Include only businesses that are open\n",
    "bus_df = bus_df[bus_df['is_open'] == 1]\n",
    "\n",
    "\n",
    "# Obtain wifi information for each business and record as 'unknown' if there is no known wifi data\n",
    "def get_wifi_info(attributes):\n",
    "    if isinstance(attributes, dict): \n",
    "        info = attributes.get('WiFi', 'unknown') \n",
    "        if info != 'unknown':\n",
    "            info = info.replace(\"u'\", \"\").replace(\"'\", \"\").strip()\n",
    "    else:\n",
    "        info = 'unknown'\n",
    "    return info\n",
    "    \n",
    "bus_df['wifi'] = bus_df['attributes'].apply(get_wifi_info)\n",
    "\n",
    "\n",
    "# Drop irrelevant columns \n",
    "drop_cols = ['is_open', 'review_count', 'attributes', 'categories']\n",
    "bus_df = bus_df.drop(drop_cols, axis =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in user reviews data\n",
    "review_json_path = '/Users/amelialei/yelp_dataset/yelp_academic_dataset_review.json'\n",
    "\n",
    "size = 1000000\n",
    "review = pd.read_json(review_json_path, lines=True, dtype = {'review_id': str, 'user_id': str, 'business_id': str, 'stars': int, 'date': str, 'text': str, 'useful': int, 'funny': int, 'cool': int}, chunksize = size)\n",
    "\n",
    "\n",
    "# Split data into chunks because the file is too large to read all at once. \n",
    "# Merge the reviews data with the businesses dataframe to create one dataframe.\n",
    "chunk_list = []\n",
    "for review_chunk in review:\n",
    "    review_chunk = review_chunk.drop(['review_id', 'useful', 'funny', 'cool'], axis=1)\n",
    "    review_chunk = review_chunk.rename(columns={'stars': 'review_stars'})\n",
    "    chunk_merged = pd.merge(bus_df, review_chunk, on='business_id', how='inner')\n",
    "    chunk_list.append(chunk_merged)\n",
    "df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering\n",
    "A collaborative filtering recommendation systems uses data on other users with similar preferences to recommend new cafes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical ratings \n",
    "df['normalized_stars'] = (df['stars'] - df['stars'].min())/(df['stars'].max()-df['stars'].min())\n",
    "df['normalized_review_stars'] = (df['review_stars'] - df['review_stars'].min())/(df['review_stars'].max()-df['review_stars'].min())\n",
    "\n",
    "# One hot encode categorical columns\n",
    "df = pd.get_dummies(df, columns=['wifi'], drop_first=True)\n",
    "\n",
    "\n",
    "# Create a user-item matrix consisting of users and their ratings for each business\n",
    "user_item_matrix = df.pivot_table(index='user_id', columns='business_id', values='review_stars')\n",
    "\n",
    "# Fill in missing ratings with the average rating for each cafe\n",
    "user_item_matrix = user_item_matrix.apply(lambda col: col.fillna(col.mean()), axis=0)\n",
    "\n",
    "# Use the nearest neighbors algorithm along with cosine similarity to measure similarity between users' preferences\n",
    "knn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "knn_model.fit(user_item_matrix.values)\n",
    "\n",
    "# Get the 6 most similar users to the given user\n",
    "user_id = '79nOboO-4_yNuQQ21EAU1A'\n",
    "distances, indices = knn_model.kneighbors([user_item_matrix.loc[user_id]], n_neighbors=6)\n",
    "\n",
    "# Exclude the given user to find the top 5 most similar users\n",
    "similar_users = indices.flatten()[1:]\n",
    "similar_users_ids = user_item_matrix.index[similar_users]\n",
    "\n",
    "# Average the similar users' cafe ratings and sort them in descending order to find the top cafe to recommend\n",
    "cafe_recs = user_item_matrix.loc[similar_users_ids].mean(axis=0).sort_values(ascending=False).head()\n",
    "print(cafe_recs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = (df['text']\n",
    "              .str.lower()\n",
    "              .replace(r'\\band\\b|\\bor\\b|\\bthe\\b|\\bis\\b|\\bto\\b', '', regex=True)\n",
    "              .replace(r'[^A-Za-z\\s]', '', regex=True))\n",
    "\n",
    "ps = PorterStemmer()\n",
    "def tokenize_and_stem(review):\n",
    "    tokens = word_tokenize(review)\n",
    "    stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "cleaned_reviews = df['text'].apply(tokenize_and_stem)\n",
    "df['text'] = cleaned_reviews\n",
    "df['sentiment'] = df['text'].apply(lambda review: TextBlob(review).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caferec-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
